from src.memory import ReplayMemory
from src.memory_palace import MemoryPalace
from src.utils import (
    set_train_path,
    set_sumo,
    import_train_configuration,
    set_load_model_path,
)
from src.model import DQN
from src.simulation import Simulation
from src.Qlearning import QSimulation
from src.base_simulation import SimulationBase
from src.visualization import Visualization
from src.intersection import Intersection
from src.accident_manager import AccidentManager
from src.comparison_utils import SimulationComparison
from src.scripts.random_demand_sides import (
    generate_and_save_random_intervals
)
import datetime
import shutil
import os

if __name__ == "__main__":
    # Load configuration
    config = import_train_configuration("config/training_testngatu6x1.yaml")
    min_epsilon = config["agent"]["min_epsilon"]
    decay_rate = config["agent"]["decay_rate"]
    start_epsilon = config["agent"]["epsilon"]

    # Create replay memory for the agent
    agent_memory_dqn = MemoryPalace(
        max_size_per_palace=config["memory_size_max"], min_size_to_sample=config["memory_size_min"]
    )

    agent_memory_q = MemoryPalace(
        max_size_per_palace=config["memory_size_max"], min_size_to_sample=config["memory_size_min"]
    )

    # Set model save path
    path = set_train_path(config["models_path_name"])

    visualization = Visualization(path=path, dpi=100)

    # Initialize accident manager
    accident_manager = AccidentManager(
        start_step=config["start_step"],
        duration=config["duration"],
        junction_id_list=config["junction_id_list"],
    )

    # Initialize simulation
    simulations_dqn = {
        loss: Simulation(
            memory=agent_memory_dqn,
            visualization=visualization,
            agent_cfg={**config["agent"], "loss_type": loss},
            max_steps=config["max_steps"],
            traffic_lights=config["traffic_lights"],
            interphase_duration=config["interphase_duration"],
            accident_manager=accident_manager,
            epoch=config["training_epochs"],
            path=path,
            training_steps=config["training_steps"],
            updating_target_network_steps=config["updating_target_network_steps"],
            save_interval=config["save_interval"],
        )
        for loss in config["agent"]["model"]["loss_type"]
    }

    # Load existing model if specified in config
    start_episode = 0

    # if config.get("load_model_name") and config["load_model_name"] is not None:
    #     model_load_path = set_load_model_path(config["models_path_name"]) + config["load_model_name"] + ".pth"
    #     checkpoint_load_path = set_load_model_path(config["models_path_name"]) + config["load_model_name"] + "_checkpoint.pth"

    #     # Try to load checkpoint first (for continuing training)
    #     try:
    #         training_state = simulation_dqn.agent.load_checkpoint(checkpoint_load_path)
    #         start_episode = training_state.get('episode', 0)
    #         start_epsilon = training_state.get('epsilon', epsilon)
    #         print(f"Successfully loaded checkpoint from: {checkpoint_load_path}")
    #         print(f"Resuming from episode {start_episode} with epsilon {start_epsilon}")
    #     except FileNotFoundError:
    #         # If checkpoint not found, try to load just the model weights
    #         try:
    #             simulation_dqn.agent.load(model_load_path, for_training=True)
    #             print(f"Successfully loaded model weights from: {model_load_path}")
    #             print("Starting training from episode 0 with fresh optimizer state")
    #         except FileNotFoundError:
    #             print(f"Model file not found: {model_load_path}. Starting with fresh model.")
    #         except Exception as e:
    #             print(f"Error loading model: {e}. Starting with fresh model.")
    #     except Exception as e:
    #         print(f"Error loading checkpoint: {e}. Starting with fresh model.")

    simulation = SimulationBase(
        max_steps=config["max_steps"],
        traffic_lights=config["traffic_lights"],
        accident_manager=accident_manager,
        visualization=visualization,
        epoch=config["training_epochs"],
        path=path,
    )

    simulation_q = QSimulation(
        memory=agent_memory_q,
        visualization=visualization,
        agent_cfg=config["agent"],
        max_steps=config["max_steps"],
        traffic_lights=config["traffic_lights"],
        interphase_duration=config["interphase_duration"],
        accident_manager=accident_manager,
        epoch=config["training_epochs"],
        path=path,
        training_steps=config["training_steps"],
        updating_target_network_steps=config["updating_target_network_steps"],
    )

    # Load existing Q-table if specified in config
    if config.get("load_q_table_name") and config["load_q_table_name"] is not None:
        q_table_load_path = (
            set_load_model_path(config["models_path_name"])
            + config["load_q_table_name"]
            + ".pkl"
        )
        simulation_q.load_q_table(q_table_load_path)

    # Initialize comparison utility
    comparison = SimulationComparison(path=path)

    episode = start_episode
    epsilon = start_epsilon
    timestamp_start = datetime.datetime.now()
    
    # Track best performance
    best_performance = {
        'episode': -1,
        'completion_rate': -1,
        'avg_travel_time': float('inf'),
        'avg_waiting_time': float('inf'),
        'combined_score': -float('inf')
    }
    performance_history = []

    while episode < config["total_episodes"]:
        print("\n----- Episode", str(episode + 1), "of", str(config["total_episodes"]))
        print("Generating routes...")
        # Run the build routes file command

        generate_and_save_random_intervals(
            sumo_cfg_file=config["sumo_cfg_file"],
            total_duration=3600,
            min_interval=360,
            max_interval=360,
            base_weight=100.0,
            high_min=200.0,
            high_max=400.0,
            min_active_sides=1,
            max_active_sides=3,
            edge_groups=config["edge_groups"],
        )

        Intersection.generate_residential_demand_routes(
            config,
            config["sumo_cfg_file"].split("/")[1],
            demand_level="low",
            enable_motorcycle=True,
            enable_passenger=True,
            enable_truck=True,
            enable_bicycle=True,
            enable_pedestrian=True,
        )
        print("Routes generated")

        # --- Run all three simulations ---
        print("Running SimulationBase (static baseline)...")
        set_sumo(config["gui"], config["sumo_cfg_file"], config["max_steps"])
        simulation_time_base = simulation.run(episode)
        print("SimulationBase time:", simulation_time_base)
        # Reset base simulation vehicle tracker after its run
        print("  Resetting vehicle tracker for base simulation")
        simulation.vehicle_tracker.reset()
        print("  Resetting history for base simulation")
        simulation.reset_history()

        # print("Running QSimulation (Q-learning)...")
        # set_sumo(config["gui"], config["sumo_cfg_file"], config["max_steps"])
        # simulation_time_q = simulation_q.run(epsilon, episode)
        # print("QSimulation time:", simulation_time_q)

        for loss_type, sim_dqn in simulations_dqn.items():
            print(f"Running DQN Simulation (loss: {loss_type})...")
            # Enable GUI once every 100 episodes for visual monitoring
            gui_enabled = (episode % 100 == 0 and episode > 0)
            set_sumo(gui_enabled, config["sumo_cfg_file"], config["max_steps"])
            if gui_enabled:
                print(f"ðŸ–¥ï¸  GUI enabled for episode {episode} - Visual monitoring")
            simulation_time_dqn, training_time_dqn = sim_dqn.run(epsilon, episode)
            print(
                f"Simulation (DQN - {loss_type}) time:",
                simulation_time_dqn,
                "Training time:",
                training_time_dqn,
            )

        epsilon = max(min_epsilon, epsilon * decay_rate)

        # --- Track Performance ---
        try:
            # Get DQN vehicle statistics
            dqn_sim = list(simulations_dqn.values())[0]  # Get first (only) DQN simulation
            dqn_stats = dqn_sim.vehicle_tracker.get_current_stats()
            
            # Debug: Print vehicle statistics
            print(f"Debug - Vehicle stats for episode {episode}:")
            print(f"  Total departed: {dqn_stats.get('total_departed', 'N/A')}")
            print(f"  Total arrived: {dqn_stats.get('total_arrived', 'N/A')}")
            print(f"  Current running: {dqn_stats.get('total_running', 'N/A')}")
            
            # Calculate performance metrics
            completion_rate = (dqn_stats['total_arrived'] / max(dqn_stats['total_departed'], 1)) * 100
            
            # Get traffic metrics (if available)
            avg_travel_time = 0
            avg_waiting_time = 0
            
            # Debug: Check if history exists and has data
            if hasattr(dqn_sim, 'history'):
                print(f"  History keys: {list(dqn_sim.history.keys())}")
                if 'travel_time' in dqn_sim.history:
                    travel_time_data = dqn_sim.history['travel_time']
                    print(f"  Travel time data available for TLs: {list(travel_time_data.keys())}")
                    for tl_id, times in travel_time_data.items():
                        print(f"    {tl_id}: {len(times)} measurements")
            else:
                print("  No history attribute found")
            
            # Try to get traffic metrics from simulation history
            if hasattr(dqn_sim, 'history') and 'travel_time' in dqn_sim.history:
                travel_times = []
                waiting_times = []
                for tl_id, times in dqn_sim.history['travel_time'].items():
                    if times:
                        travel_times.extend(times[-10:])  # Last 10 measurements
                for tl_id, times in dqn_sim.history['waiting_time'].items():
                    if times:
                        waiting_times.extend(times[-10:])  # Last 10 measurements
                
                avg_travel_time = sum(travel_times) / len(travel_times) if travel_times else 0
                avg_waiting_time = sum(waiting_times) / len(waiting_times) if waiting_times else 0
            
            # Combined score (higher is better)
            # Weight: completion_rate (60%) - travel_time penalty (20%) - waiting_time penalty (20%)
            combined_score = (completion_rate * 0.6) - (avg_travel_time * 0.2) - (avg_waiting_time * 0.2)
            
            # Store performance data
            current_performance = {
                'episode': episode,
                'completion_rate': completion_rate,
                'avg_travel_time': avg_travel_time,
                'avg_waiting_time': avg_waiting_time,
                'combined_score': combined_score,
                'epsilon': epsilon,
                'total_departed': dqn_stats['total_departed'],
                'total_arrived': dqn_stats['total_arrived']
            }
            performance_history.append(current_performance)
            
            # Check if this is the best performance so far
            if combined_score > best_performance['combined_score']:
                # Clean up old best model files first
                model_save_name = config.get("save_model_name", "dqn_model")
                
                # Remove old best files if they exist
                if best_performance['episode'] != -1:  # If there was a previous best
                    old_best_model = path + f"{model_save_name}_BEST_ep{best_performance['episode']}.pth"
                    old_best_checkpoint = path + f"{model_save_name}_BEST_ep{best_performance['episode']}_checkpoint.pth"
                    
                    try:
                        if os.path.exists(old_best_model):
                            os.remove(old_best_model)
                            print(f"   ðŸ—‘ï¸  Removed old best model: ep{best_performance['episode']}")
                        if os.path.exists(old_best_checkpoint):
                            os.remove(old_best_checkpoint)
                            print(f"   ðŸ—‘ï¸  Removed old best checkpoint: ep{best_performance['episode']}")
                    except Exception as e:
                        print(f"   âš ï¸  Could not remove old best files: {e}")
                
                # Update best performance record
                best_performance = current_performance.copy()
                print(f"\nðŸ† NEW BEST PERFORMANCE at Episode {episode}!")
                print(f"   Completion Rate: {completion_rate:.1f}%")
                print(f"   Avg Travel Time: {avg_travel_time:.2f}")
                print(f"   Avg Waiting Time: {avg_waiting_time:.2f}")
                print(f"   Combined Score: {combined_score:.2f}")
                
                # Create new best model files
                for loss_type, sim_dqn in simulations_dqn.items():
                    # Source files (current episode checkpoint if it exists)
                    current_model_path = path + f"{model_save_name}_episode_{episode}.pth"
                    current_checkpoint_path = path + f"{model_save_name}_episode_{episode}_checkpoint.pth"
                    
                    # New best model files
                    best_model_path = path + f"{model_save_name}_BEST_ep{episode}.pth"
                    best_checkpoint_path = path + f"{model_save_name}_BEST_ep{episode}_checkpoint.pth"
                    
                    # Copy files if they exist, otherwise save current state
                    if os.path.exists(current_model_path):
                        shutil.copy2(current_model_path, best_model_path)
                        print(f"   âœ… Best model copied: ep{episode}")
                    else:
                        # Save current state if regular checkpoint doesn't exist yet
                        sim_dqn.agent.save(best_model_path)
                        print(f"   âœ… Best model saved: ep{episode}")
                    
                    if os.path.exists(current_checkpoint_path):
                        shutil.copy2(current_checkpoint_path, best_checkpoint_path)
                        print(f"   âœ… Best checkpoint copied: ep{episode}")
                    else:
                        # Save current state if regular checkpoint doesn't exist yet
                        sim_dqn.agent.save_checkpoint(best_checkpoint_path, episode=episode, epsilon=epsilon)
                        print(f"   âœ… Best checkpoint saved: ep{episode}")
                    
                    # Always update the "CURRENT" best files (overwrite)
                    current_best_model = path + f"{model_save_name}_BEST_CURRENT.pth"
                    current_best_checkpoint = path + f"{model_save_name}_BEST_CURRENT_checkpoint.pth"
                    
                    sim_dqn.agent.save(current_best_model)
                    sim_dqn.agent.save_checkpoint(current_best_checkpoint, episode=episode, epsilon=epsilon)
                    print(f"   âœ… Current best updated: ep{episode}")
                    
                    break  # Only need to do this once since there's only one DQN simulation
            
            # Print current vs best performance every 10 episodes
            if episode % 10 == 0:
                print(f"\nðŸ“Š Performance Summary - Episode {episode}")
                print(f"Current: Score={combined_score:.2f}, Completion={completion_rate:.1f}%, Travel={avg_travel_time:.2f}, Wait={avg_waiting_time:.2f}")
                print(f"Best:    Score={best_performance['combined_score']:.2f}, Episode={best_performance['episode']}, Completion={best_performance['completion_rate']:.1f}%")
                
        except AttributeError as e:
            if 'num_atoms' in str(e):
                print(f"âš ï¸  Model compatibility error: {e}")
                print("   This error indicates the DQN model is missing the 'num_atoms' attribute for C51 distributional DQN.")
                print("   The model should be updated to include: self.num_atoms = 51")
            else:
                print(f"âš ï¸  Attribute error tracking performance: {e}")
            # Continue training even if performance tracking fails
        except Exception as e:
            print(f"âš ï¸  Error tracking performance: {e}")
            # Continue training even if performance tracking fails
        
        # Reset vehicle trackers after performance tracking is complete
        for loss_type, sim_dqn in simulations_dqn.items():
            print(f"  Resetting vehicle tracker for DQN simulation")
            sim_dqn.vehicle_tracker.reset()
            print(f"  Resetting history for DQN simulation")
            sim_dqn.reset_history()
            break  # Only need to do this once since there's only one DQN simulation

        # --- Save comparison plots ---
        print("Saving comparison plots...")
        if episode % 10 == 0:
            print("Generating plots at episode", episode, "...")
            visualization.save_plot(
                episode=episode,
                metrics=[
                    "density_avg",
                    "travel_time_avg",
                    "outflow_avg",
                    "travel_delay_avg",
                    "waiting_time_avg",
                    "queue_length_avg",
                ],
                names=["dqn_qr", "dqn_mse", "dqn_huber", "dqn_weighted", "q", "base"],
            )
            print("Plots at episode", episode, "generated")

            # --- Generate traffic light comparison tables ---
            print("Generating traffic light comparison tables...")
            try:
                comparison_results = comparison.save_comparison_tables(episode)
                comparison.print_comparison_summary(episode)
                print("Traffic light comparison tables generated successfully")
            except Exception as e:
                print(f"Error generating comparison tables: {e}")
                print(
                    "Comparison tables will be generated when CSV files are available"
                )

            # --- Generate vehicle comparison from logs ---
            print("Generating vehicle comparison from logs...")
            try:
                visualization.create_vehicle_comparison_from_logs(episode, ["dqn", "base"])  # Only compare DQN and Base since Q is disabled
                print("Vehicle comparison from logs generated successfully")
            except Exception as e:
                print(f"Error generating vehicle comparison from logs: {e}")
                print("Vehicle comparison will be generated when log files are available")

        # Save model at specified intervals
        save_interval = config.get("save_interval", 10)  # Default to every 10 episodes
        if episode % save_interval == 0 and episode > 0:
            model_save_name = config.get("save_model_name", "dqn_model")

            for loss_type, sim_dqn in simulations_dqn.items():
                # Save model weights with simple DQN naming (since only running QR)
                model_save_path = path + f"{model_save_name}_episode_{episode}.pth"
                sim_dqn.agent.save(model_save_path)

                # Save complete checkpoint for continuing training
                checkpoint_save_path = path + f"{model_save_name}_episode_{episode}_checkpoint.pth"
                sim_dqn.agent.save_checkpoint(checkpoint_save_path, episode=episode, epsilon=epsilon)

                print(f"DQN model saved at episode {episode}: {model_save_path}")
                print(f"DQN checkpoint saved at episode {episode}: {checkpoint_save_path}")

            # Save Q-learning Q-table (only if Q-learning is running)
            # Note: Uncomment when Q-learning is enabled
            # q_table_save_path = path + f"q_table_{model_save_name}_episode_{episode}.pkl"
            # simulation_q.save_q_table(path, episode)

        episode += 1

    print("\n----- Start time:", timestamp_start)
    print("----- End time:", datetime.datetime.now())
    print("----- Session info saved at:", path)
    
    # Print best performance summary
    print(f"\nðŸ† BEST PERFORMANCE SUMMARY")
    print("=" * 50)
    print(f"Best Episode: {best_performance['episode']}")
    print(f"Best Completion Rate: {best_performance['completion_rate']:.2f}%")
    print(f"Best Travel Time: {best_performance['avg_travel_time']:.2f}")
    print(f"Best Waiting Time: {best_performance['avg_waiting_time']:.2f}")
    print(f"Best Combined Score: {best_performance['combined_score']:.2f}")
    print(f"Vehicles: {best_performance['total_arrived']}/{best_performance['total_departed']}")
    
    # Save performance history to CSV
    if performance_history:
        import pandas as pd
        performance_df = pd.DataFrame(performance_history)
        performance_csv = path + "performance_history.csv"
        performance_df.to_csv(performance_csv, index=False)
        print(f"ðŸ“ˆ Performance history saved: {performance_csv}")
        
        # Create performance plot
        try:
            import matplotlib.pyplot as plt
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('Training Performance Over Time', fontsize=16)
            
            episodes = performance_df['episode']
            
            # Plot 1: Completion Rate
            ax1.plot(episodes, performance_df['completion_rate'], 'b-', linewidth=2)
            ax1.axhline(y=best_performance['completion_rate'], color='r', linestyle='--', alpha=0.7)
            ax1.set_xlabel('Episode')
            ax1.set_ylabel('Completion Rate (%)')
            ax1.set_title('Vehicle Completion Rate')
            ax1.grid(True, alpha=0.3)
            ax1.text(0.02, 0.98, f"Best: {best_performance['completion_rate']:.1f}% (Ep {best_performance['episode']})", 
                     transform=ax1.transAxes, verticalalignment='top', bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
            
            # Plot 2: Combined Score
            ax2.plot(episodes, performance_df['combined_score'], 'g-', linewidth=2)
            ax2.axhline(y=best_performance['combined_score'], color='r', linestyle='--', alpha=0.7)
            ax2.set_xlabel('Episode')
            ax2.set_ylabel('Combined Score')
            ax2.set_title('Overall Performance Score')
            ax2.grid(True, alpha=0.3)
            ax2.text(0.02, 0.98, f"Best: {best_performance['combined_score']:.1f} (Ep {best_performance['episode']})", 
                     transform=ax2.transAxes, verticalalignment='top', bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
            
            # Plot 3: Travel Time
            ax3.plot(episodes, performance_df['avg_travel_time'], 'orange', linewidth=2)
            if best_performance['avg_travel_time'] > 0:
                ax3.axhline(y=best_performance['avg_travel_time'], color='r', linestyle='--', alpha=0.7)
            ax3.set_xlabel('Episode')
            ax3.set_ylabel('Avg Travel Time')
            ax3.set_title('Average Travel Time')
            ax3.grid(True, alpha=0.3)
            
            # Plot 4: Epsilon Decay
            ax4.plot(episodes, performance_df['epsilon'], 'purple', linewidth=2)
            ax4.set_xlabel('Episode')
            ax4.set_ylabel('Epsilon')
            ax4.set_title('Exploration Rate (Epsilon)')
            ax4.grid(True, alpha=0.3)
            
            plt.tight_layout()
            performance_plot = path + "performance_history.png"
            plt.savefig(performance_plot, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"ðŸ“Š Performance plot saved: {performance_plot}")
            
        except Exception as e:
            print(f"âš ï¸  Could not create performance plot: {e}")

    print(f"Saving final model at {path}...")
    # model_save_name = config.get("save_model_name", "dqn_model")

    # # Save final model weights
    # model_final_path = path + f"{model_save_name}_final.pth"
    # simulation_dqn.agent.save(model_final_path)

    # # Save final checkpoint
    # checkpoint_final_path = path + f"{model_save_name}_final_checkpoint.pth"
    # simulation_dqn.agent.save_checkpoint(checkpoint_final_path, episode=episode, epsilon=epsilon)

    # print(f"Final DQN model saved: {model_final_path}")
    # print(f"Final DQN checkpoint saved: {checkpoint_final_path}")

    # Save final Q-table
    simulation_q.save_q_table(path, episode="final")
    print(f"Final Q-table saved")
    print("---------------------------------------")

    print("Generating plots...")
    # We simple by averaging the history over all traffic lights
    avg_history = {}

    for metric, data_per_tls in simulations_dqn.history.items():
        # Transpose the list of lists into per-timestep values
        # Filter out missing/empty lists first
        data_lists = [data for data in data_per_tls.values() if len(data) > 0]

        if not data_lists:
            continue  # Skip if no data

        # Truncate to minimum length to avoid index errors
        min_length = min(len(data) for data in data_lists)
        data_lists = [data[:min_length] for data in data_lists]

        # Average per timestep
        avg_data = [sum(step_vals) / len(step_vals) for step_vals in zip(*data_lists)]

        # Save to avg_history
        avg_history[metric] = avg_data

    for metric, data in avg_history.items():
        visualization.save_data_and_plot(
            data=data,
            filename=f"{metric}_avg",
            xlabel="Step",
            ylabel=metric.replace("_", " ").title(),
        )

    print("Plots generated")